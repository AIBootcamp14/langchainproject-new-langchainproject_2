"""
íŠ¸ë Œë“œ ë¶„ì„ ë„êµ¬
Naver DataLab APIë¥¼ í™œìš©í•œ ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„
"""
import json
import logging
import os
import re
import statistics
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests

from app.tools.llm import call_llm_with_context

logger = logging.getLogger(__name__)

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
NAVER_DATALAB_URL = os.getenv(
    "NAVER_DATALAB_URL",
    "https://openapi.naver.com/v1/datalab/search",
)

STOPWORDS = {
    "ìµœê·¼",
    "ìš”ì¦˜",
    "íŠ¸ë Œë“œ",
    "trend",
    "ë¶„ì„",
    "ì•Œë ¤ì¤˜",
    "í•´ì£¼ì„¸ìš”",
    "í•´ì¤˜",
    "ë°ì´í„°",
    "ì‹œì¥",
    "ì–´ë–»ê²Œ",
    "ìš”ì²­",
    "ë³´ê³ ",
    "ì •ë³´",
    "ê´€ë ¨",
    "ëŒ€í•œ",
    "ì…ë‹ˆë‹¤",
    "ì£¼ì„¸ìš”",
    "please",
    "tell",
    "show",
    "about",
    "analysis",
    "ì •ë¦¬",
    "í•´ì¤˜ìš”",
    "ì•Œë ¤ì£¼ì„¸ìš”",
}


def extract_trend_keyword(user_message: str, fallback_to_llm: bool = True) -> Optional[str]:
    """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤."""
    if not user_message:
        return None

    text = user_message.strip()
    if not text:
        return None

    quoted_matches = re.findall(r"[\"â€œâ€'â€˜â€™]([^\"â€œâ€'â€˜â€™]{2,})[\"â€œâ€'â€˜â€™]", text)
    for candidate in quoted_matches:
        cleaned = _clean_keyword(candidate)
        if cleaned:
            return cleaned

    hashtags = re.findall(r"#([A-Za-z0-9ê°€-í£]+)", text)
    if hashtags:
        cleaned = _clean_keyword(hashtags[0])
        if cleaned:
            return cleaned

    patterns = (
        r"(?P<keyword>[ê°€-í£A-Za-z0-9&\s]+?)\s*(?:íŠ¸ë Œë“œ|trend)\s*(?:ë¶„ì„|ì•Œë ¤ì¤˜|ë°ì´í„°|í˜„í™©|ë³´ê³ |íŒŒì•…)?",
        r"(?P<keyword>[ê°€-í£A-Za-z0-9&\s]+?)\s*(?:ì‹œì¥|ìˆ˜ìš”)\s*(?:ì „ë§|ë¶„ì„|ì–´ë–»ê²Œ|ì¶”ì´)",
        r"(?P<keyword>[ê°€-í£A-Za-z0-9&\s]+?)\s*(?:ì— ëŒ€í•œ|ê´€ë ¨)\s*(?:íŠ¸ë Œë“œ|ë¶„ì„)",
    )
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            cleaned = _clean_keyword(match.group("keyword"))
            if cleaned:
                return cleaned

    tokens = [token for token in re.split(r"[,\s]+", text) if token]
    filtered = [token for token in tokens if _is_meaningful_token(token)]
    if filtered:
        candidate = " ".join(filtered[:2]) if len(filtered) > 1 else filtered[0]
        cleaned = _clean_keyword(candidate)
        if cleaned:
            return cleaned

    if fallback_to_llm:
        return _extract_keyword_with_llm(text)

    return None


def resolve_time_window(user_message: str) -> Dict[str, Any]:
    """ì‚¬ìš©ì ë¬¸ì¥ì—ì„œ ë¶„ì„ ê¸°ê°„ì„ ì¶”ì •í•œë‹¤."""
    now = datetime.utcnow()
    default_days = 180
    default = {
        "start_date": (now - timedelta(days=default_days)).strftime("%Y-%m-%d"),
        "end_date": now.strftime("%Y-%m-%d"),
        "time_unit": "week",
        "days": default_days,
    }

    if not user_message:
        return default

    text = user_message.lower()
    days: Optional[int] = None

    match = re.search(r"(\d+)\s*(?:ì¼|ì¼ê°„|ì¼ë™ì•ˆ|days?)", text)
    if match:
        days = max(1, int(match.group(1)))

    if days is None:
        match = re.search(r"(\d+)\s*(?:ì£¼|ì£¼ê°„|weeks?)", text)
        if match:
            days = int(match.group(1)) * 7

    if days is None:
        match = re.search(r"(\d+)\s*(?:ê°œì›”|ë‹¬|months?)", text)
        if match:
            days = int(match.group(1)) * 30

    if days is None:
        match = re.search(r"(\d+)\s*(?:ë…„|years?)", text)
        if match:
            days = int(match.group(1)) * 365

    if days is None:
        condensed = text.replace(" ", "")
        if "ë¶„ê¸°" in text:
            days = 90
        elif "ë°˜ë…„" in text:
            days = 180
        elif "1ë…„" in condensed or "ì¼ë…„" in condensed:
            days = 365
        elif "3ë…„" in condensed:
            days = 365 * 3
        elif "5ë…„" in condensed:
            days = 365 * 5

    if days is None:
        return default

    days = max(7, min(days, 365 * 10))
    start = (now - timedelta(days=days)).strftime("%Y-%m-%d")
    time_unit = "date"
    if days > 120:
        time_unit = "week"
    if days > 365 * 2:
        time_unit = "month"

    return {
        "start_date": start,
        "end_date": now.strftime("%Y-%m-%d"),
        "time_unit": time_unit,
        "days": days,
    }


def get_naver_datalab_trends(
    keywords: List[str],
    start_date: str,
    end_date: str,
    time_unit: str = "date",
) -> Dict[str, Any]:
    """Naver DataLab APIë¡œ íŠ¸ë Œë“œ ì¡°íšŒ (ì‹¤íŒ¨ ì‹œ ëª¨ì˜ ë°ì´í„°)."""
    logger.info("Naver DataLab ì¡°íšŒ: %s (%s~%s)", keywords, start_date, end_date)

    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
        payload = {
            "startDate": start_date,
            "endDate": end_date,
            "timeUnit": time_unit,
            "keywordGroups": [
                {"groupName": kw, "keywords": [kw]}
                for kw in keywords[:5]
            ],
            "device": "",
            "gender": "",
            "ages": [],
        }
        try:
            response = requests.post(
                NAVER_DATALAB_URL,
                headers={
                    "X-Naver-Client-Id": NAVER_CLIENT_ID,
                    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
                    "Content-Type": "application/json",
                },
                json=payload,
                timeout=10,
            )
            response.raise_for_status()
            data = response.json()
            results = data.get("results")
            if results:
                normalized: Dict[str, Any] = {
                    "keywords": keywords[:5],
                    "period": {"start": start_date, "end": end_date},
                    "time_unit": time_unit,
                    "results": [],
                    "is_mock": False,
                }
                for entry in results:
                    series = [
                        {"date": item.get("period"), "value": item.get("ratio")}
                        for item in entry.get("data", [])
                        if item.get("ratio") is not None
                    ]
                    normalized["results"].append(
                        {
                            "group": entry.get("title") or entry.get("groupName") or keywords[0],
                            "keywords": entry.get("keywords", []),
                            "series": series,
                        }
                    )
                return normalized
        except requests.RequestException as exc:
            logger.warning("Naver DataLab API í˜¸ì¶œ ì‹¤íŒ¨: %s", exc)
        except ValueError as exc:
            logger.warning("Naver DataLab ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: %s", exc)

    return _generate_mock_naver_trends(keywords, start_date, end_date, time_unit)


def analyze_trend_data(trend_data: Dict[str, Any]) -> Dict[str, Any]:
    """ìˆ˜ì§‘í•œ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ í•µì‹¬ ì§€í‘œì™€ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•œë‹¤."""
    logger.info("íŠ¸ë Œë“œ ë°ì´í„° ë¶„ì„ ì‹œì‘")

    keyword = trend_data.get("keyword")
    start_date = trend_data.get("start_date")
    end_date = trend_data.get("end_date")
    time_unit = trend_data.get("time_unit")

    naver_data = trend_data.get("naver") or {}

    naver_series: List[Dict[str, Any]] = []
    if naver_data:
        if "results" in naver_data:
            for entry in naver_data["results"]:
                naver_series.extend(entry.get("series", []))
        elif "data" in naver_data:
            for item in naver_data["data"]:
                naver_series.append({"date": item.get("period"), "value": item.get("ratio")})

    naver_metrics = _compute_series_metrics(naver_series)

    if naver_metrics["has_data"]:
        summary_lines = [
            f"- ê²€ìƒ‰ ì§€ìˆ˜ {naver_metrics['momentum_label']} íë¦„ (ìµœê·¼ ë³€í™” {format_percentage(naver_metrics['momentum_pct'])})",
            f"- ì²« ì‹œì  ëŒ€ë¹„ ë³€í™”ìœ¨ {format_percentage(naver_metrics['growth_pct'])}",
        ]
    else:
        summary_lines = ["- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]

    signal = _infer_signal(naver_metrics)
    insight = _generate_insights_with_llm(
        keyword,
        summary_lines,
        naver_metrics,
        time_unit,
        start_date,
        end_date,
    )
    if not insight:
        insight = _generate_rule_based_insight(keyword, naver_metrics)

    confidence = "ë†’ìŒ" if naver_metrics["has_data"] and not naver_data.get("is_mock", True) else (
        "ì¤‘ê°„" if naver_metrics["has_data"] else "ë‚®ìŒ"
    )

    return {
        "keyword": keyword,
        "start_date": start_date,
        "end_date": end_date,
        "time_unit": time_unit,
        "naver": {
            **naver_metrics,
            "time_unit": naver_data.get("time_unit"),
            "is_mock": naver_data.get("is_mock", True),
        },
        "summary": "\n".join(summary_lines),
        "insight": insight,
        "signal": signal,
        "confidence": confidence,
    }


def format_percentage(value: Optional[float]) -> str:
    if isinstance(value, (int, float)):
        return f"{value:+.1f}%"
    return "N/A"


def _normalize_whitespace(value: str) -> str:
    return re.sub(r"\s+", " ", value.strip())


def _clean_keyword(keyword: str) -> Optional[str]:
    if not keyword:
        return None
    cleaned = _normalize_whitespace(keyword)
    cleaned = re.sub(r"(?:íŠ¸ë Œë“œ|trend|ë¶„ì„|ì‹œì¥|ë°ì´í„°)$", "", cleaned, flags=re.IGNORECASE).strip()
    cleaned = re.sub(r"^(?:ìµœê·¼|ìš”ì¦˜|ëŒ€í•œ|ê´€ë ¨|êµ­ë‚´|í•´ì™¸)\s+", "", cleaned).strip(' "\'()[]')
    if len(cleaned) < 2:
        return None
    if cleaned.lower() in STOPWORDS:
        return None
    return cleaned


def _is_meaningful_token(token: str) -> bool:
    stripped = token.strip(' "\'()[]{}.,!?')
    if not stripped:
        return False
    lower = stripped.lower()
    if lower in STOPWORDS:
        return False
    if "íŠ¸ë Œë“œ" in lower or "trend" in lower or "ë¶„ì„" in lower:
        return False
    if len(stripped) == 1 and not stripped.isdigit():
        return False
    return True


def _extract_keyword_with_llm(text: str) -> Optional[str]:
    try:
        response = call_llm_with_context(
            [
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. "
                        "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë¬¸ì¥ì—ì„œ ë¶„ì„í•  í•µì‹¬ í‚¤ì›Œë“œ ë˜ëŠ” ì œí’ˆëª…ì„ í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. "
                        "ë¶ˆí•„ìš”í•œ ì„¤ëª…ê³¼ ë”°ì˜´í‘œëŠ” ì œê±°í•˜ì„¸ìš”."
                    ),
                },
                {"role": "user", "content": text},
            ]
        )
        if response.get("success"):
            raw = response.get("reply_text", "").strip()
            if not raw:
                return None
            keyword = raw.splitlines()[0]
            return _clean_keyword(keyword)
    except Exception as exc:
        logger.debug("LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: %s", exc)
    return None


def _generate_mock_naver_trends(
    keywords: List[str],
    start_date: str,
    end_date: str,
    time_unit: str,
) -> Dict[str, Any]:
    logger.info("Naver DataLab ëª¨ì˜ ë°ì´í„° ì‚¬ìš©: %s", keywords)
    start_dt = _parse_date(start_date) or (datetime.utcnow() - timedelta(days=90))
    end_dt = _parse_date(end_date) or datetime.utcnow()
    if end_dt <= start_dt:
        end_dt = start_dt + timedelta(days=30)

    total_days = max(1, (end_dt - start_dt).days)
    if time_unit == "date":
        steps = max(8, min(20, total_days // 5 or 8))
        step_days = max(1, total_days // max(steps - 1, 1))
    elif time_unit == "week":
        steps = max(8, min(20, total_days // 7 or 8))
        step_days = 7
    else:
        steps = max(6, min(20, total_days // 30 or 6))
        step_days = 30

    results = []
    for idx, keyword in enumerate(keywords[:5]):
        series = []
        baseline = 40 + (hash((keyword, idx)) % 40)
        slope = (hash((keyword, "trend")) % 5) - 2
        for i in range(steps):
            dt = start_dt + timedelta(days=i * step_days)
            if dt > end_dt:
                dt = end_dt
            jitter = (hash((keyword, i)) % 10) - 5
            value = max(5, min(100, baseline + slope * (i - steps // 2) + jitter))
            series.append({"date": dt.strftime("%Y-%m-%d"), "value": value})
        results.append(
            {
                "group": keyword,
                "keywords": [keyword],
                "series": series,
            }
        )

    return {
        "keywords": keywords[:5],
        "period": {"start": start_date, "end": end_date},
        "time_unit": time_unit,
        "results": results,
        "is_mock": True,
    }


def _compute_series_metrics(series: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not series:
        return {
            "has_data": False,
            "data_points": 0,
            "average": None,
            "latest_value": None,
            "latest_date": None,
            "growth_pct": None,
            "momentum_pct": None,
            "momentum_label": "ë°ì´í„° ë¶€ì¡±",
            "peak": None,
            "volatility": None,
            "series_tail": [],
            "first_value": None,
        }

    cleaned = []
    for idx, point in enumerate(series):
        value = point.get("value")
        if value is None:
            continue
        try:
            numeric = float(value)
        except (TypeError, ValueError):
            continue
        date_obj = _parse_date(point.get("date"))
        cleaned.append((date_obj or (datetime.min + timedelta(days=idx)), idx, point.get("date"), numeric))

    if not cleaned:
        return {
            "has_data": False,
            "data_points": 0,
            "average": None,
            "latest_value": None,
            "latest_date": None,
            "growth_pct": None,
            "momentum_pct": None,
            "momentum_label": "ë°ì´í„° ë¶€ì¡±",
            "peak": None,
            "volatility": None,
            "series_tail": [],
            "first_value": None,
        }

    cleaned.sort(key=lambda item: (item[0], item[1]))
    ordered_dates = [item[2] for item in cleaned]
    values = [item[3] for item in cleaned]

    data_points = len(values)
    average = sum(values) / data_points
    first_value = values[0]
    latest_value = values[-1]
    growth_pct = ((latest_value - first_value) / first_value * 100) if first_value else None

    window = min(3, data_points)
    early_avg = sum(values[:window]) / window if window else None
    recent_avg = sum(values[-window:]) / window if window else None
    momentum_pct = ((recent_avg - early_avg) / early_avg * 100) if early_avg else None
    momentum_label = _momentum_label(momentum_pct)

    peak_index = max(range(data_points), key=lambda i: values[i])
    peak = {"date": ordered_dates[peak_index], "value": values[peak_index]}

    try:
        volatility = statistics.stdev(values) if data_points > 1 else 0.0
    except statistics.StatisticsError:
        volatility = 0.0

    series_tail = [
        {"date": ordered_dates[i], "value": values[i]}
        for i in range(max(0, data_points - 5), data_points)
    ]

    return {
        "has_data": True,
        "data_points": data_points,
        "average": average,
        "latest_value": latest_value,
        "latest_date": ordered_dates[-1],
        "growth_pct": growth_pct,
        "momentum_pct": momentum_pct,
        "momentum_label": momentum_label,
        "peak": peak,
        "volatility": volatility,
        "series_tail": series_tail,
        "first_value": first_value,
    }


def _parse_date(value: Optional[str]) -> Optional[datetime]:
    if value is None:
        return None
    if isinstance(value, datetime):
        return value
    try:
        return datetime.fromisoformat(str(value).replace("Z", "+00:00"))
    except ValueError:
        pass
    for fmt in ("%Y-%m-%d", "%Y%m%d"):
        try:
            return datetime.strptime(str(value), fmt)
        except ValueError:
            continue
    return None


def _momentum_label(momentum_pct: Optional[float]) -> str:
    if not isinstance(momentum_pct, (int, float)):
        return "ë°ì´í„° ë¶€ì¡±"
    if momentum_pct > 5:
        return "ìƒìŠ¹"
    if momentum_pct < -5:
        return "í•˜ë½"
    return "ë³´í•©"


def _infer_signal(metrics: Dict[str, Any]) -> str:
    if not metrics.get("has_data"):
        return "ë°ì´í„° ë¶€ì¡±"

    score = 0.0
    weight = 0.0

    growth = metrics.get("growth_pct")
    momentum = metrics.get("momentum_pct")
    latest = metrics.get("latest_value")

    if isinstance(growth, (int, float)):
        score += growth * 0.6
        weight += 0.6
    if isinstance(momentum, (int, float)):
        score += momentum * 0.4
        weight += 0.4
    if isinstance(latest, (int, float)) and latest >= 70:
        score += 5

    if weight == 0:
        return "ë°ì´í„° ë¶€ì¡±"

    normalized = score / weight
    if normalized >= 20:
        return "ğŸš€ ê°•í•œ ìƒìŠ¹ì„¸"
    if normalized >= 8:
        return "â†—ï¸ ì™„ë§Œí•œ ìƒìŠ¹"
    if normalized <= -20:
        return "ğŸ“‰ ê°•í•œ í•˜ë½"
    if normalized <= -8:
        return "â†˜ï¸ ì™„ë§Œí•œ í•˜ë½"
    return "â– ë³´í•©ì„¸"


def _generate_insights_with_llm(
    keyword: Optional[str],
    summary_lines: List[str],
    naver_metrics: Dict[str, Any],
    time_unit: Optional[str],
    start_date: Optional[str],
    end_date: Optional[str],
) -> Optional[str]:
    if not keyword:
        return None

    try:
        metrics_brief = {
            "naver": {
                "average": naver_metrics.get("average"),
                "growth_pct": naver_metrics.get("growth_pct"),
                "momentum_pct": naver_metrics.get("momentum_pct"),
                "latest_value": naver_metrics.get("latest_value"),
                "peak": naver_metrics.get("peak"),
            }
        }

        summary_text = "\n".join(summary_lines)

        response = call_llm_with_context(
            [
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµê°€ì…ë‹ˆë‹¤. Naver ê²€ìƒ‰ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ "
                        "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ì™€ ëŒ€ì‘ ì „ëµì„ ê°„ê²°í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"í‚¤ì›Œë“œ: {keyword}\n"
                        f"ê¸°ê°„: {start_date} ~ {end_date} (ë‹¨ìœ„: {time_unit})\n"
                        f"ìš”ì•½: {summary_text}\n"
                        f"ì§€í‘œ: {json.dumps(metrics_brief, ensure_ascii=False)}\n"
                        "ì¦ê°€/ê°ì†Œ ì›ì¸ê³¼ ëŒ€ì‘ ì „ëµì„ bullet 2-3ê°œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
                    ),
                },
            ]
        )

        if response.get("success"):
            return response.get("reply_text", "").strip() or None
    except Exception as exc:
        logger.debug("LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: %s", exc)

    return None


def _generate_rule_based_insight(
    keyword: Optional[str],
    naver_metrics: Dict[str, Any],
) -> str:
    if not naver_metrics.get("has_data"):
        return "ìœ ì˜ë¯¸í•œ ê²€ìƒ‰ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ë„“í˜€ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ë³´ì„¸ìš”."

    lines: List[str] = []
    momentum_label = naver_metrics.get("momentum_label")
    momentum_pct = format_percentage(naver_metrics.get("momentum_pct"))
    growth_pct = format_percentage(naver_metrics.get("growth_pct"))

    lines.append(
        f"'{keyword}' ê²€ìƒ‰ ì§€ìˆ˜ëŠ” ìµœê·¼ {momentum_label} íë¦„ì´ë©° ë‹¨ê¸° ë³€í™”ìœ¨ì€ {momentum_pct}ì…ë‹ˆë‹¤."
    )
    lines.append(
        f"ë¶„ì„ ì‹œì‘ ì‹œì  ëŒ€ë¹„ ë³€í™”ìœ¨ì€ {growth_pct}ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ê²€ìƒ‰ í”¼í¬ ì‹œì ê³¼ ìµœì‹  ì§€ìˆ˜ë¥¼ ì°¸ê³ í•´ ì½˜í…ì¸  íƒ€ì´ë°ì„ ì¡°ì •í•˜ì„¸ìš”."
    )
    lines.append("ìƒìŠ¹ êµ¬ê°„ì—ì„œëŠ” ìº í˜ì¸ì„ í™•ëŒ€í•˜ê³ , í•˜ë½ êµ­ë©´ì—ì„œëŠ” ì—°ê´€ í‚¤ì›Œë“œë¥¼ ë°œêµ´í•´ ê´€ì‹¬ì„ ìœ ì§€í•˜ì„¸ìš”.")
    return "\n".join(lines)
